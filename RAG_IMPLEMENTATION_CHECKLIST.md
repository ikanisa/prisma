# RAG Ingestion Pipeline - Implementation Checklist

## âœ… Pre-Implementation (Verify Before Starting)

- [ ] Supabase project exists with DATABASE_URL
- [ ] OpenAI API key obtained
- [ ] Node.js 22+ installed
- [ ] pnpm 9.12.3+ installed
- [ ] knowledge_web_sources table populated (200 URLs)

## âœ… Files Delivered (Verify All Present)

### Database
- [ ] `supabase/migrations/20260201160000_rag_ingestion_pipeline.sql` (7.4K)

### Code
- [ ] `scripts/ingestKnowledgeFromWeb.ts` (12K)
- [ ] `.github/workflows/rag-ingestion.yml` (2.7K)
- [ ] `scripts/verify-rag-setup.sh` (2.1K)

### Configuration
- [ ] `package.json` updated with:
  - [ ] `ingest:web` script
  - [ ] `@supabase/supabase-js` dependency
  - [ ] `openai` dependency
  - [ ] `node-fetch@2` dependency
  - [ ] `jsdom` dependency
  - [ ] `pdf-parse` dependency
  - [ ] `js-sha256` dependency
  - [ ] TypeScript types (@types/jsdom, @types/pdf-parse)

### Documentation
- [ ] `RAG_INGESTION_PIPELINE_QUICKSTART.md` (6.6K)
- [ ] `RAG_INGESTION_PIPELINE_README.md` (8.7K)
- [ ] `RAG_INGESTION_PIPELINE_SUMMARY.md` (9.9K)
- [ ] `RAG_INGESTION_PIPELINE_INDEX.md` (14K)
- [ ] `RAG_DELIVERY_SUMMARY.txt` (9.8K)
- [ ] `RAG_IMPLEMENTATION_CHECKLIST.md` (this file)

## âœ… Installation Steps

- [ ] 1. Install dependencies: `pnpm install --frozen-lockfile`
- [ ] 2. Verify dependencies installed: `pnpm list openai jsdom pdf-parse`
- [ ] 3. Run verification script: `./scripts/verify-rag-setup.sh`

## âœ… Database Setup

- [ ] 1. Apply migration:
  ```bash
  psql "$DATABASE_URL" -f supabase/migrations/20260201160000_rag_ingestion_pipeline.sql
  ```
- [ ] 2. Verify tables created:
  ```sql
  \dt knowledge_web_pages
  \dt knowledge_chunks
  ```
- [ ] 3. Verify function created:
  ```sql
  \df deep_search_knowledge
  ```
- [ ] 4. Verify pgvector enabled:
  ```sql
  select * from pg_extension where extname = 'vector';
  ```
- [ ] 5. Check pre-seeded pages:
  ```sql
  select count(*) from knowledge_web_pages;
  -- Expected: 200
  ```

## âœ… Environment Variables

- [ ] Set `SUPABASE_URL`:
  ```bash
  export SUPABASE_URL=https://your-project.supabase.co
  ```
- [ ] Set `SUPABASE_SERVICE_ROLE_KEY`:
  ```bash
  export SUPABASE_SERVICE_ROLE_KEY=eyJhbGc...
  ```
- [ ] Set `OPENAI_API_KEY`:
  ```bash
  export OPENAI_API_KEY=sk-...
  ```
- [ ] Optional: Set `DATABASE_URL` (for monitoring):
  ```bash
  export DATABASE_URL=postgresql://...
  ```

## âœ… First Ingestion Run

- [ ] 1. Run ingestion worker:
  ```bash
  pnpm run ingest:web
  ```
- [ ] 2. Verify output shows:
  - [ ] "Starting knowledge ingestion from web sources..."
  - [ ] "Found X pages to process"
  - [ ] Processing logs for each URL
  - [ ] "Ingested X chunks for URL..."
  - [ ] "Ingestion run completed successfully!"
- [ ] 3. Check database for chunks:
  ```sql
  select count(*) from knowledge_chunks;
  -- Expected: 100-300 chunks from first 25 URLs
  ```
- [ ] 4. Check page statuses:
  ```sql
  select status, count(*) from knowledge_web_pages group by status;
  -- Expected: ~25 ACTIVE, rest pending
  ```

## âœ… Full Ingestion (All 200 URLs)

- [ ] Run ingestion 8 times (200 Ã· 25 = 8 batches):
  ```bash
  for i in {1..8}; do
    echo "=== Batch $i/8 ==="
    pnpm run ingest:web
    sleep 60
  done
  ```
- [ ] Verify all pages processed:
  ```sql
  select status, count(*) from knowledge_web_pages group by status;
  -- Expected: ~190-200 ACTIVE, 0-10 ERROR
  ```
- [ ] Verify chunk counts:
  ```sql
  select count(*) from knowledge_chunks;
  -- Expected: 1500-3000 chunks (varies by content)
  ```

## âœ… GitHub Actions Setup

- [ ] 1. Add GitHub secrets:
  - [ ] `SUPABASE_URL`
  - [ ] `SUPABASE_SERVICE_ROLE_KEY`
  - [ ] `OPENAI_API_KEY`
  - [ ] `DATABASE_URL` (optional, for stats)
- [ ] 2. Verify workflow file exists:
  ```bash
  ls .github/workflows/rag-ingestion.yml
  ```
- [ ] 3. Enable workflow in GitHub UI:
  - [ ] Actions tab â†’ RAG Ingestion Pipeline â†’ Enable
- [ ] 4. Test manual trigger:
  - [ ] Actions â†’ RAG Ingestion Pipeline â†’ Run workflow
- [ ] 5. Verify scheduled run (next day at 2 AM UTC):
  - [ ] Check Actions tab for automated run

## âœ… Testing

### Unit Tests
- [ ] Test deep_search_knowledge() function:
  ```sql
  select * from deep_search_knowledge(
    query_embedding := (select embedding from knowledge_chunks limit 1),
    p_category := 'TAX',
    p_jurisdiction := 'RW',
    p_limit := 5
  );
  ```

### Integration Test
- [ ] Create test script (TypeScript):
  ```typescript
  // Test embedding â†’ search â†’ results flow
  const embedding = await openai.embeddings.create({
    model: "text-embedding-3-large",
    input: "What is the VAT rate in Rwanda?"
  });
  
  const { data } = await supabase.rpc('deep_search_knowledge', {
    query_embedding: embedding.data[0].embedding,
    p_category: 'TAX',
    p_jurisdiction: 'RW',
    p_limit: 5
  });
  
  console.log(data); // Should return Rwanda tax chunks
  ```

### Agent Integration Test
- [ ] Integrate deep_search_knowledge() into Rwanda Tax Agent
- [ ] Test query: "What is the VAT rate for exported services in Rwanda?"
- [ ] Verify agent uses RAG context in response
- [ ] Verify agent cites sources from knowledge_chunks

## âœ… Monitoring Setup

- [ ] Create monitoring view:
  ```sql
  create or replace view rag_ingestion_stats as
  select 
    (select count(*) from knowledge_web_pages where status = 'ACTIVE') as active_pages,
    (select count(*) from knowledge_web_pages where status = 'ERROR') as error_pages,
    (select count(*) from knowledge_chunks) as total_chunks,
    (select count(distinct category) from knowledge_chunks) as categories,
    (select count(distinct jurisdiction_code) from knowledge_chunks) as jurisdictions,
    (select max(last_fetched_at) from knowledge_web_pages) as last_ingestion_at,
    (select count(*) from knowledge_web_pages where last_fetched_at < now() - interval '30 days') as stale_pages;
  ```
- [ ] Bookmark monitoring queries:
  - [ ] `select * from rag_ingestion_stats;`
  - [ ] `select status, count(*) from knowledge_web_pages group by status;`
  - [ ] `select url, fetch_error from knowledge_web_pages where status = 'ERROR';`

## âœ… Documentation Review

- [ ] Team reads `RAG_INGESTION_PIPELINE_QUICKSTART.md`
- [ ] DevOps reviews `RAG_INGESTION_PIPELINE_README.md`
- [ ] Tech lead reviews `RAG_INGESTION_PIPELINE_SUMMARY.md`
- [ ] Team bookmarks `RAG_INGESTION_PIPELINE_INDEX.md`

## âœ… Production Readiness

- [ ] Migration applied successfully
- [ ] Dependencies installed
- [ ] Environment variables set
- [ ] First ingestion run successful
- [ ] All 200 URLs ingested
- [ ] GitHub Action scheduled and tested
- [ ] Monitoring queries working
- [ ] Agent integration tested
- [ ] Team trained on system
- [ ] Runbook documented (see README)

## âœ… Post-Deployment

- [ ] Monitor first automated run (2 AM UTC next day)
- [ ] Review GitHub Action logs
- [ ] Check for errors in knowledge_web_pages
- [ ] Verify chunk growth over time
- [ ] Monitor OpenAI API costs
- [ ] Collect agent feedback on search quality

## âœ… Maintenance Schedule

### Daily
- [ ] Review GitHub Action logs (automated)

### Weekly
- [ ] Check for ERROR status pages
- [ ] Review stale pages (not fetched in 7+ days)

### Monthly
- [ ] Audit chunk distribution across categories
- [ ] Review OpenAI API costs
- [ ] Update source URLs if any deprecated

### Quarterly
- [ ] Force re-ingest all sources (refresh content)
- [ ] Review embedding model (newer versions available?)
- [ ] Analyze search quality with agent feedback

## ðŸŽ‰ Success Criteria

- âœ… All 200 URLs ingested into knowledge_chunks
- âœ… pgvector search returns relevant results
- âœ… Agents successfully use deep_search_knowledge()
- âœ… GitHub Action runs daily without errors
- âœ… Monitoring dashboards accessible
- âœ… Team trained and documentation complete

---

**Total estimated setup time**: 30-60 minutes  
**Total estimated cost**: $0.26 initial + $0.03/month incremental  
**Status**: âœ… Production Ready
