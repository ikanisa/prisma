# ğŸ“ AI AGENT LEARNING SYSTEM - START HERE

> **Transform your AI agents from static tools into continuously evolving, self-improving intelligent systems**

## ğŸš€ What is This?

The Agent Learning System is Prisma Glow's comprehensive framework for collecting feedback, training improvements, and safely deploying better agent versions. It's the "brain behind the brain" - making your agents smarter every day.

## âœ¨ Quick Value Proposition

- **ğŸ“Š Collect Feedback**: Simple thumbs up/down, detailed ratings, user corrections
- **ğŸ‘¨â€ğŸ« Expert Review**: Quality scoring and annotation workflow
- **ğŸ¤– Automated Learning**: Prompt optimization, RAG tuning, behavioral cloning
- **ğŸ§ª A/B Testing**: Safe, data-driven deployments with automatic rollback
- **ğŸ“ˆ Continuous Improvement**: Agents get better with every interaction

## ğŸ¯ 5-Minute Quick Start

### Step 1: Add Feedback to Your Agent UI

```tsx
import { FeedbackCollector } from '@/components/learning/FeedbackCollector';

// Inside your agent execution result component
<AgentResponse response={execution.response} />

<FeedbackCollector
  executionId={execution.id}
  agentId={execution.agentId}
  agentOutput={execution.response.text}
  onFeedbackSubmitted={() => {
    toast.success('Thank you for helping improve our agents!');
  }}
/>
```

**What users see**:
- Quick thumbs up/down buttons
- Click "Give detailed feedback" for:
  - 5-star rating
  - Accuracy/helpfulness/clarity scores
  - Issue categories
  - Text feedback
  - Output correction editor

### Step 2: Check the Stats

```tsx
import { useLearningStats } from '@/hooks/learning/useLearning';

function LearningDashboard() {
  const { data: stats } = useLearningStats();
  
  return (
    <div>
      <MetricCard 
        label="Pending Annotations" 
        value={stats.pendingAnnotations} 
      />
      <MetricCard 
        label="Annotated Today" 
        value={stats.annotatedToday} 
      />
      <MetricCard 
        label="Active Training Runs" 
        value={stats.activeTrainingRuns} 
      />
    </div>
  );
}
```

### Step 3: Start Your First Training Run

```tsx
import { useStartTraining } from '@/hooks/learning/useLearning';

const { mutateAsync: startTraining } = useStartTraining();

// When you have ~100 approved examples
await startTraining({
  agentId: 'tax-agent-123',
  datasetId: 'approved-corrections-v1',
  trainingType: 'prompt_optimization',
  config: {
    optimization_goals: ['accuracy', 'completeness', 'clarity']
  }
});
```

### Step 4: Run Your First A/B Test

```tsx
import { useCreateExperiment } from '@/hooks/learning/useLearning';

const { mutateAsync: createExperiment } = useCreateExperiment();

const experiment = await createExperiment({
  agentId: 'tax-agent-123',
  name: 'Improved Tax Calculation Prompt',
  description: 'Testing clearer instructions for complex calculations',
  hypothesis: 'New prompt increases accuracy by 15% and reduces corrections by 10%',
  controlConfig: {
    system_prompt: currentPrompt
  },
  treatmentConfig: {
    system_prompt: optimizedPrompt
  }
});

// Start the experiment (splits traffic 50/50)
await startExperiment(experiment.id);
```

## ğŸ“š Documentation Structure

```
.
â”œâ”€â”€ README_AGENT_LEARNING.md (This file)                 - Quick start
â”œâ”€â”€ docs/AGENT_LEARNING_SYSTEM_GUIDE.md                  - Comprehensive guide
â””â”€â”€ AGENT_LEARNING_IMPLEMENTATION_COMPLETE.md            - Implementation status
```

**Choose your path**:
- **Just want to get started?** â†’ Continue reading below
- **Need full technical details?** â†’ Read `docs/AGENT_LEARNING_SYSTEM_GUIDE.md`
- **Want to verify implementation?** â†’ Check `AGENT_LEARNING_IMPLEMENTATION_COMPLETE.md`

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEARNING PIPELINE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. User Feedback â†’ FeedbackCollector Component             â”‚
â”‚           â†“                                                  â”‚
â”‚  2. Data Storage â†’ learning_examples table                  â”‚
â”‚           â†“                                                  â”‚
â”‚  3. Expert Review â†’ Annotation Queue                        â”‚
â”‚           â†“                                                  â”‚
â”‚  4. Training Dataset â†’ Approved examples                    â”‚
â”‚           â†“                                                  â”‚
â”‚  5. Learning Engine â†’ Prompt/RAG/Behavior optimizer         â”‚
â”‚           â†“                                                  â”‚
â”‚  6. Evaluation â†’ A/B test, regression test                  â”‚
â”‚           â†“                                                  â”‚
â”‚  7. Deployment â†’ Canary â†’ Gradual rollout â†’ Full            â”‚
â”‚           â†“                                                  â”‚
â”‚  8. Monitoring â†’ Metrics, rollback triggers                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Learning Types

| Type | Frequency | Human Oversight | Use Case |
|------|-----------|-----------------|----------|
| **Prompt Learning** | Continuous (hourly batches) | Review before deploy | Optimize system prompts, few-shot examples |
| **RAG Learning** | Daily | Automated + spot checks | Improve retrieval, chunking strategy |
| **Behavior Learning** | Weekly | Required approval | Learn from expert demonstrations |
| **Fine-Tuning** | Monthly | Full review | Specialized model updates |
| **Reinforcement Learning** | Experimental | Research team | Reward-based policy optimization |

## ğŸ› ï¸ Implementation Checklist

### âœ… Already Complete

- [x] Database schema (6 tables with RLS policies)
- [x] Backend API (9 endpoints)
- [x] Learning engines (Prompt, RAG, Behavior)
- [x] Frontend components (FeedbackCollector)
- [x] React hooks (useLearning)
- [x] Comprehensive documentation

### â³ Your Next Steps

- [ ] **Week 1**: Add FeedbackCollector to 3 most-used agents
- [ ] **Week 2**: Create annotation dashboard at `/admin/learning/annotation`
- [ ] **Week 3**: Train expert reviewers, approve 50+ examples
- [ ] **Week 4**: Run first optimization, deploy with canary

## ğŸ¨ UI Components Available

### FeedbackCollector

```tsx
<FeedbackCollector
  executionId={string}        // Required: execution ID
  agentId={string}            // Required: agent ID
  agentOutput={string}        // Required: agent's response text
  onFeedbackSubmitted={() => void}  // Optional: callback
/>
```

**Features**:
- Quick feedback (thumbs up/down)
- Detailed feedback dialog:
  - Overall star rating (1-5)
  - Dimension ratings (accuracy, helpfulness, clarity, completeness)
  - Issue categorization (8 categories)
  - Free-text feedback
  - Output correction editor
- Visual feedback states
- Loading states during submission

### React Hooks

```tsx
// Submit feedback
const submitFeedback = useSubmitFeedback();
await submitFeedback.mutateAsync({...});

// Get annotation queue
const { data: queue } = useAnnotationQueue({ domain: 'tax' });

// Submit annotation
const submitAnnotation = useSubmitAnnotation();
await submitAnnotation.mutateAsync({...});

// Get stats
const { data: stats } = useLearningStats();

// Training runs
const { data: runs } = useTrainingRuns(agentId);
const startTraining = useStartTraining();

// Experiments
const { data: experiments } = useLearningExperiments(agentId);
const createExperiment = useCreateExperiment();
```

## ğŸ”Œ API Endpoints

### Feedback Collection
```http
POST /api/learning/feedback
Content-Type: application/json

{
  "executionId": "uuid",
  "agentId": "uuid",
  "feedbackType": "correction",
  "rating": 4,
  "feedbackText": "Good but missing details on...",
  "correctionText": "Corrected output...",
  "issueCategories": ["incomplete"],
  "dimensions": {
    "accuracy": 5,
    "helpfulness": 4,
    "clarity": 4,
    "completeness": 3
  }
}
```

### Get Statistics
```http
GET /api/learning/stats

Response:
{
  "pendingAnnotations": 42,
  "annotatedToday": 15,
  "activeTrainingRuns": 2,
  "activeExperiments": 1
}
```

### Start Training
```http
POST /api/learning/training/start
Content-Type: application/json

{
  "agentId": "uuid",
  "datasetId": "uuid",
  "trainingType": "prompt_optimization",
  "config": {
    "optimization_goals": ["accuracy", "completeness"]
  }
}
```

### Create Experiment
```http
POST /api/learning/experiments
Content-Type: application/json

{
  "agentId": "uuid",
  "name": "Test new approach",
  "description": "...",
  "hypothesis": "...",
  "controlConfig": {...},
  "treatmentConfig": {...}
}
```

## ğŸ“ˆ Metrics to Track

### Quality Metrics
- **User Satisfaction**: % of ratings >= 4 stars (target: >80%)
- **Correction Rate**: % of outputs corrected by users (target: <5%)
- **Task Completion**: % of tasks completed successfully (target: >90%)
- **Expert Approval**: % of examples approved by experts (target: >75%)

### Performance Metrics
- **Latency**: p95 response time (target: <2s)
- **Error Rate**: % of failed executions (target: <1%)
- **Retrieval Accuracy**: % of relevant docs retrieved (target: >90%)
- **Token Efficiency**: Cost per task (track trend)

### Business Metrics
- **User Adoption**: % of users regularly using agents
- **Task Automation**: % of manual tasks now automated
- **Cost Savings**: $ saved vs manual processes
- **ROI**: Return on AI investment

## ğŸ›¡ï¸ Safety Guardrails

### Automatic Rollback Triggers
System automatically rolls back to previous version if:
- User satisfaction drops >10%
- Error rate increases >20%
- Latency increases >50%
- Safety violations detected (harmful/inappropriate outputs)

### Human Oversight Required
- **Prompt Learning**: Expert review before deployment
- **Behavior Learning**: Required approval for all changes
- **Fine-Tuning**: Full review cycle with test results
- **Experiments**: Review hypothesis and configs

### Quality Gates
- Minimum quality score: 0.8/1.0
- Minimum sample size for experiments: 1,000 interactions
- Minimum duration for A/B tests: 1 week
- Statistical significance: p < 0.05

## ğŸ¯ Success Metrics

### Phase 1: Foundation (âœ… COMPLETE)
- âœ… Database schema deployed
- âœ… API endpoints implemented
- âœ… Frontend components created
- âœ… Documentation written

### Phase 2: Adoption (IN PROGRESS - You Are Here)
- â³ 80% of agents have feedback collection enabled
- â³ 5+ expert reviewers trained
- â³ 500+ approved training examples
- â³ First successful optimization deployed

### Phase 3: Scale (NEXT)
- â³ Automated daily training pipeline
- â³ 10+ successful A/B tests
- â³ 20%+ improvement in user satisfaction
- â³ 50%+ reduction in correction rate

### Phase 4: Excellence (FUTURE)
- â³ Self-improving agents
- â³ Cross-agent knowledge transfer
- â³ Industry-leading quality scores
- â³ Continuous innovation pipeline

## ğŸ’¡ Best Practices

### âœ… DO
- Start with 1-2 high-value agents
- Collect diverse examples across all capabilities
- Require expert review for all deployments
- Use A/B testing for significant changes
- Monitor metrics closely during rollout
- Maintain golden test datasets
- Document all changes and decisions

### âŒ DON'T
- Deploy without evaluation
- Train on unreviewed user feedback
- Skip safety validation
- Over-fit to recent examples
- Fine-tune too frequently (causes instability)
- Ignore user complaints
- Rush through review process

## ğŸ› Troubleshooting

### Low Feedback Volume
**Problem**: Not enough user feedback  
**Solutions**:
- Add prominent feedback buttons
- Send reminder emails
- Gamify feedback (leaderboards, badges)
- Simplify feedback UI (one-click thumbs)

### Poor Quality Annotations
**Problem**: Inconsistent expert reviews  
**Solutions**:
- Create annotation guidelines
- Train reviewers with examples
- Implement inter-rater reliability checks
- Provide feedback to reviewers

### Training Doesn't Improve Quality
**Problem**: Metrics don't improve after training  
**Solutions**:
- Check data quality (remove noise)
- Verify evaluation metrics are correct
- Look for distribution shift
- Try different learning approaches

### Deployment Degraded Metrics
**Problem**: Quality drops after deployment  
**Solutions**:
- Immediate rollback (automatic or manual)
- Review evaluation coverage
- Check for edge cases
- Improve regression test suite

## ğŸ“ Support

- **Full Documentation**: [`docs/AGENT_LEARNING_SYSTEM_GUIDE.md`](docs/AGENT_LEARNING_SYSTEM_GUIDE.md)
- **Implementation Status**: [`AGENT_LEARNING_IMPLEMENTATION_COMPLETE.md`](AGENT_LEARNING_IMPLEMENTATION_COMPLETE.md)
- **API Reference**: `/api/docs#learning` (when server running)
- **Team Chat**: #agent-learning
- **Email**: learning@prismaglow.com

## ğŸ‰ Ready to Get Started?

1. **Add feedback collection** to your first agent (copy code from Quick Start above)
2. **Collect 100+ feedback samples** in the first week
3. **Set up annotation workflow** for expert review
4. **Run your first training** when you have 50+ approved examples
5. **Deploy with A/B testing** and monitor closely

---

**System Status**: âœ… Production Ready  
**Last Updated**: January 28, 2025  
**Version**: 1.0.0  
**Maintained By**: AI Platform Team

**Questions?** Start with this README, then dive into the comprehensive guide for details.
