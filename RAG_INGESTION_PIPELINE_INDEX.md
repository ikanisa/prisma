# RAG Ingestion Pipeline - Complete Index

**Status**: âœ… **PRODUCTION READY**  
**Last Updated**: 2025-12-01  
**Version**: 1.0.0

---

## ğŸ“ File Structure

### Core Implementation

| File | Description | Lines |
|------|-------------|-------|
| `supabase/migrations/20260201160000_rag_ingestion_pipeline.sql` | Database schema: tables, indexes, RLS, RPC functions | 250 |
| `scripts/ingestKnowledgeFromWeb.ts` | TypeScript worker: fetch â†’ chunk â†’ embed â†’ store | 380 |
| `.github/workflows/rag-ingestion.yml` | GitHub Action: scheduled daily runs + monitoring | 95 |
| `scripts/verify-rag-setup.sh` | Setup verification script | 75 |

### Documentation

| File | Purpose | Audience |
|------|---------|----------|
| `RAG_INGESTION_PIPELINE_QUICKSTART.md` | 5-minute setup guide | Developers (first-time setup) |
| `RAG_INGESTION_PIPELINE_README.md` | Complete technical documentation | Developers/DevOps |
| `RAG_INGESTION_PIPELINE_SUMMARY.md` | Implementation summary with examples | Tech leads/PMs |
| `RAG_INGESTION_PIPELINE_INDEX.md` | This file - navigation hub | All roles |

### Configuration

| File | Change | Purpose |
|------|--------|---------|
| `package.json` | Added `ingest:web` script + 7 dependencies | Run worker via `pnpm run ingest:web` |

---

## ğŸ¯ Quick Navigation

### I want to...

**Set up the system for the first time**  
â†’ Start here: [`RAG_INGESTION_PIPELINE_QUICKSTART.md`](./RAG_INGESTION_PIPELINE_QUICKSTART.md)

**Understand the architecture**  
â†’ Read: [`RAG_INGESTION_PIPELINE_README.md`](./RAG_INGESTION_PIPELINE_README.md) - Architecture section

**Run ingestion manually**  
â†’ Command: `pnpm run ingest:web`  
â†’ Docs: [`RAG_INGESTION_PIPELINE_QUICKSTART.md`](./RAG_INGESTION_PIPELINE_QUICKSTART.md) - Step 5

**Schedule automated ingestion**  
â†’ Setup: [`.github/workflows/rag-ingestion.yml`](./.github/workflows/rag-ingestion.yml)  
â†’ Docs: [`RAG_INGESTION_PIPELINE_README.md`](./RAG_INGESTION_PIPELINE_README.md) - Scheduling section

**Integrate with AI agents**  
â†’ Code examples: [`RAG_INGESTION_PIPELINE_SUMMARY.md`](./RAG_INGESTION_PIPELINE_SUMMARY.md) - Agent Integration  
â†’ RPC function: `deep_search_knowledge()` in migration file

**Monitor ingestion status**  
â†’ SQL queries: [`RAG_INGESTION_PIPELINE_README.md`](./RAG_INGESTION_PIPELINE_README.md) - Monitoring section

**Troubleshoot issues**  
â†’ Guide: [`RAG_INGESTION_PIPELINE_QUICKSTART.md`](./RAG_INGESTION_PIPELINE_QUICKSTART.md) - Troubleshooting section

**Verify setup**  
â†’ Run: `./scripts/verify-rag-setup.sh`

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Knowledge Web Sources                       â”‚
â”‚                    (200 URLs from YAML)                         â”‚
â”‚   IFRS, ISA, ACCA, RRA, BNR, OECD, Big4, Tax Authorities       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Ingestion Worker (TypeScript)                  â”‚
â”‚  â€¢ Fetch HTML/PDF     â€¢ Extract text      â€¢ Chunk (~4000 chars) â”‚
â”‚  â€¢ Generate embeddings (OpenAI)           â€¢ Store in Supabase   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Supabase PostgreSQL                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ knowledge_web_pages                                       â”‚ â”‚
â”‚  â”‚ â€¢ URL tracking  â€¢ SHA-256 hash  â€¢ Status  â€¢ Errors       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                           â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ knowledge_chunks (pgvector)                               â”‚ â”‚
â”‚  â”‚ â€¢ Text chunks  â€¢ 1536-dim embeddings  â€¢ Category/juris   â”‚ â”‚
â”‚  â”‚ â€¢ IVFFlat index for fast similarity search               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                           â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ deep_search_knowledge(query_embedding, filters)           â”‚ â”‚
â”‚  â”‚ â€¢ Semantic search  â€¢ Category/jurisdiction filters        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       AI Agents                                 â”‚
â”‚  â€¢ Rwanda Tax Agent    â€¢ IFRS Audit Agent   â€¢ ACCA Agent       â”‚
â”‚  â€¢ Query embeddings â†’ deep_search â†’ Context â†’ GPT-4 response   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Data Flow

### 1. Ingestion Flow

```
URL â†’ HTTP GET â†’ Raw Content (HTML/PDF)
  â†’ SHA-256 hash (change detection)
  â†’ Text Extraction (jsdom / pdf-parse)
  â†’ Chunking (~4000 chars, sentence boundaries)
  â†’ Embedding (OpenAI text-embedding-3-large)
  â†’ Storage (knowledge_chunks table)
  â†’ Update metadata (knowledge_web_pages)
```

### 2. Query Flow

```
User Query â†’ Embed (OpenAI)
  â†’ deep_search_knowledge(embedding, category, jurisdiction)
  â†’ pgvector cosine similarity search
  â†’ Top N chunks
  â†’ Agent context â†’ GPT-4 completion
```

---

## ğŸ“Š Database Schema

### `knowledge_web_pages`
**Purpose**: Track ingested content from each URL

| Column | Type | Description |
|--------|------|-------------|
| `id` | uuid | Primary key |
| `source_id` | uuid | FK to knowledge_web_sources |
| `url` | text | URL to fetch |
| `title` | text | Extracted page title |
| `status` | text | ACTIVE / INACTIVE / ERROR |
| `http_status` | int | HTTP response code |
| `content_type` | text | text/html, application/pdf |
| `sha256_hash` | text | Content hash (change detection) |
| `last_fetched_at` | timestamptz | Last ingestion timestamp |
| `fetch_error` | text | Error message if failed |

**Indexes**: source_id, url, status, last_fetched_at

### `knowledge_chunks`
**Purpose**: Vector store for RAG search

| Column | Type | Description |
|--------|------|-------------|
| `id` | bigserial | Primary key |
| `source_id` | uuid | FK to knowledge_web_sources |
| `page_id` | uuid | FK to knowledge_web_pages |
| `chunk_index` | int | 0, 1, 2... ordering within page |
| `content` | text | Chunk text (~4000 chars) |
| `tokens` | int | Token count (optional) |
| `category` | text | TAX, AUDIT, IFRS, etc. |
| `jurisdiction_code` | text | RW, GLOBAL, MT, etc. |
| `tags` | text[] | Tags from source |
| `embedding` | vector(1536) | OpenAI embedding |

**Indexes**: 
- Unique: (page_id, chunk_index)
- Foreign keys: source_id, page_id
- Filters: category, jurisdiction_code
- **Vector index**: IVFFlat (lists=100) for cosine similarity

### `deep_search_knowledge()` RPC
**Signature**:
```sql
deep_search_knowledge(
  query_embedding vector(1536),
  p_category text default null,
  p_jurisdiction text default null,
  p_tags text[] default null,
  p_limit int default 20
)
```

**Returns**: Chunks ordered by similarity with metadata (source name, URL)

---

## ğŸ’» Commands

### Setup

```bash
# Verify setup
./scripts/verify-rag-setup.sh

# Install dependencies
pnpm install --frozen-lockfile

# Apply migration
psql "$DATABASE_URL" -f supabase/migrations/20260201160000_rag_ingestion_pipeline.sql
```

### Ingestion

```bash
# Run once (processes 25 URLs)
pnpm run ingest:web

# Run full batch (all 200 URLs)
for i in {1..8}; do pnpm run ingest:web; sleep 60; done

# Force re-ingestion (clear hashes)
psql "$DATABASE_URL" -c "update knowledge_web_pages set sha256_hash = null;"
```

### Monitoring

```sql
-- Status overview
select status, count(*) from knowledge_web_pages group by status;

-- Chunk statistics
select category, jurisdiction_code, count(*) 
from knowledge_chunks 
group by category, jurisdiction_code;

-- Recent errors
select url, fetch_error, last_fetched_at 
from knowledge_web_pages 
where status = 'ERROR' 
order by last_fetched_at desc limit 10;

-- Stale pages (30+ days)
select url, last_fetched_at 
from knowledge_web_pages 
where last_fetched_at < now() - interval '30 days';
```

---

## ğŸ”‘ Environment Variables

| Variable | Required | Purpose | Example |
|----------|----------|---------|---------|
| `SUPABASE_URL` | âœ… | Supabase project URL | `https://abc.supabase.co` |
| `SUPABASE_SERVICE_ROLE_KEY` | âœ… | Service role key (full access) | `eyJhbGc...` |
| `OPENAI_API_KEY` | âœ… | OpenAI API key for embeddings | `sk-...` |
| `DATABASE_URL` | âš ï¸  | Direct Postgres URL (monitoring only) | `postgresql://...` |

---

## ğŸ“… Scheduling Options

### Option 1: GitHub Actions (Recommended)
- **File**: `.github/workflows/rag-ingestion.yml`
- **Schedule**: Daily at 2 AM UTC (`cron: '0 2 * * *'`)
- **Setup**: Add secrets to GitHub repo
- **Monitoring**: Action output shows stats + errors

### Option 2: pg_cron (Supabase)
```sql
select cron.schedule('rag-ingestion', '0 2 * * *', $$
  select net.http_post(
    url := 'https://project.supabase.co/functions/v1/ingest-knowledge',
    headers := '{"Authorization": "Bearer SERVICE_ROLE_KEY"}'
  )
$$);
```

### Option 3: Cloud Run + Cloud Scheduler
- Deploy worker as Cloud Run job
- Trigger via Cloud Scheduler

---

## ğŸ’° Costs

### OpenAI Embeddings
- **Model**: text-embedding-3-large
- **Price**: $0.13 per 1M tokens
- **Initial**: 200 URLs Ã— 10 chunks Ã— 1000 tokens = **$0.26**
- **Incremental**: ~10% change monthly = **$0.03/month**

### Supabase Storage
- **Vector size**: 6 KB per 1536-dim vector
- **Total**: 2000 chunks Ã— 6 KB = **12 MB** (negligible)

**Total monthly cost**: ~$0.05-0.10 (mostly OpenAI)

---

## ğŸ§ª Testing

### Integration Test
```typescript
// Test full pipeline
import { createClient } from '@supabase/supabase-js';
import OpenAI from 'openai';

const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY);
const openai = new OpenAI({ apiKey: OPENAI_API_KEY });

// 1. Embed query
const { data: [{ embedding }] } = await openai.embeddings.create({
  model: "text-embedding-3-large",
  input: "What is the VAT rate in Rwanda?"
});

// 2. Search
const { data, error } = await supabase.rpc('deep_search_knowledge', {
  query_embedding: embedding,
  p_category: 'TAX',
  p_jurisdiction: 'RW',
  p_limit: 5
});

console.log(data); // Should return Rwanda tax chunks
```

---

## ğŸ› ï¸ Maintenance Tasks

### Weekly
- Review GitHub Action logs for ingestion errors
- Check stale pages (not fetched in 30+ days)

### Monthly
- Audit chunk distribution across categories
- Review and update source URLs if any are deprecated
- Check for duplicate chunks (rare but possible)

### Quarterly
- Re-ingest all sources to refresh content
- Review embedding model (newer models may be available)
- Analyze search quality (agent feedback)

---

## ğŸ”— Integration Points

### Related Systems
- **Knowledge Web Sources**: YAML registry â†’ `knowledge_web_sources` table
- **Agent Learning**: Feedback loop â†’ classification improvements
- **Auto-Classification**: Ground truth â†’ auto-categorization
- **Accounting KB**: Domain knowledge â†’ GL/tax mappings

### Agent Consumers
- Rwanda Tax Agent (RW + TAX)
- IFRS Audit Agent (GLOBAL + IFRS)
- ACCA Agent (GLOBAL + ACCA)
- Malta Corporate Agent (MT + CORPORATE)

---

## âœ… Deployment Checklist

- [ ] Migration applied (`knowledge_web_pages`, `knowledge_chunks` exist)
- [ ] Dependencies installed (`pnpm list | grep openai`)
- [ ] Environment variables set in GitHub secrets
- [ ] First ingestion run successful (25+ pages)
- [ ] Verify chunks: `select count(*) from knowledge_chunks;`
- [ ] Test search: `select * from deep_search_knowledge(...);`
- [ ] GitHub Action enabled and scheduled
- [ ] Monitoring queries bookmarked
- [ ] Documentation reviewed by team

---

## ğŸ“š Additional Resources

- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Supabase Vector Search](https://supabase.com/docs/guides/ai/vector-search)
- [GitHub Actions Scheduling](https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#schedule)

---

**Questions or issues?** Check the troubleshooting section in [`RAG_INGESTION_PIPELINE_QUICKSTART.md`](./RAG_INGESTION_PIPELINE_QUICKSTART.md)
